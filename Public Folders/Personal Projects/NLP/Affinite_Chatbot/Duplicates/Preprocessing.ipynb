{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "290a5c00-67f5-4024-8f81-b778efb054b3",
   "metadata": {},
   "source": [
    "how to: https://www.youtube.com/watch?v=RpWeNzfSUHw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbea1a87-c58e-43f9-9caa-158a18a7685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/princemarcelle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/princemarcelle/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#stem words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "    print(\"NLTK data downloaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK data: {e}\")\n",
    "\n",
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc715ffc-a2b9-427e-93d1-aef3281717e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    Split sentence into array of words/tokens\n",
    "    A token can be a word or punctuation character, or number\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return nltk.word_tokenize(sentence)\n",
    "    except LookupError:\n",
    "        print(\"NLTK data not found. Downloading...\")\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('punkt_tab')\n",
    "        return nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6987fddd-22b8-44c1-b701-bd13280a9569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    \"\"\"\n",
    "    Stemming = find the root form of the word\n",
    "    examples:\n",
    "    words = [\"organize\", \"organizes\", \"organizing\"]\n",
    "    words = [stem(w) for w in words]\n",
    "    -> [\"organ\", \"organ\", \"organ\"]\n",
    "    \"\"\"\n",
    "    return stemmer.stem(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e1af303-8bf4-46f9-8f14-5d9bf4aa5d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tokenized_sentence, words):\n",
    "    \"\"\"\n",
    "    Return bag of words array:\n",
    "    1 for each known word that exists in the sentence, 0 otherwise\n",
    "    example:\n",
    "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
    "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "    bag   = [  0 ,    1 ,    0 ,   1 ,    0 ,     0 ,      0]\n",
    "    \"\"\"\n",
    "    # Stem each word\n",
    "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
    "    # Initialize bag with 0 for each word\n",
    "    bag = np.zeros(len(words), dtype=np.float32)\n",
    "    for idx, w in enumerate(words):\n",
    "        if w in sentence_words: \n",
    "            bag[idx] = 1\n",
    "\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdc3e7a8-7f4a-4a41-82df-d20194ebb054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'long', 'is', 'your', 'walk', 'home', '?']\n"
     ]
    }
   ],
   "source": [
    "#test tokenization\n",
    "a = 'How long is your walk home?'\n",
    "a = tokenize(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90895157-5be6-405c-80c0-3500daff5bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['organ', 'organ', 'organ']\n"
     ]
    }
   ],
   "source": [
    "#test steming\n",
    "words = ['Trying', 'try', 'tries']\n",
    "words2 = ['Organize', 'organizes', 'organizing']\n",
    "stemmed_words = [stem(w) for w in words2]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2361a2c5-d0b0-48b1-be17-0938aade70ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "#test bag of words\n",
    "sentence = ['Trying', 'to', 'get', 'a', 'bottle']\n",
    "words = ['Trying', 'to', 'hi']\n",
    "bag = bag_of_words(sentence, words)\n",
    "print(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d7e63-4c3f-4393-b879-fe433b27a048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
